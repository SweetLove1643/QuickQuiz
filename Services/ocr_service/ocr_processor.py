import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
from typing import List
import logging

logger = logging.getLogger(__name__)


class OCRProcessor:
    def __init__(self):
        self.device = self._get_device()
        self.dtype = torch.float16 if self.device == "cuda" else torch.float32
        self.model_id = "Qwen/Qwen2-VL-2B-Instruct"

        logger.info("üîÑ Loading Qwen2-VL model... Please wait.")
        self._load_model()

    def _get_device(self):
        if torch.cuda.is_available():
            logger.info("üöÄ GPU CUDA detected ‚Üí using GPU")
            return "cuda"
        logger.info("üñ• Running on CPU")
        return "cpu"

    def _load_model(self):
        try:
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.model_id, torch_dtype="auto", device_map="auto"
            )
            self.model.eval()

            self.processor = AutoProcessor.from_pretrained(self.model_id)

            logger.info("‚úÖ Qwen2-VL model loaded successfully")

        except Exception as e:
            logger.error(f"‚ùå Error loading model: {e}")
            raise e
    
    def estimate_output_tokens(self, image: Image.Image) -> int:
        width, height = image.size
        print(f'DEBUG: K√≠ch th∆∞·ªõc h√¨nh ·∫£nh ƒë·∫ßu v√†o l√†: {width}x{height}')
        num_pixels = width * height
        if num_pixels <= 512 * 512:
            print(f'DEBUG: Max token l√†: 256')
            return 256
        elif num_pixels <= 720 * 720:
            print(f'DEBUG: Max token l√†: 512')
            return 512
        elif num_pixels <= 1024 * 1024:
            print(f'DEBUG: Max token l√†: 1024')
            return 1024
        elif num_pixels <= 1536 * 1536:
            print(f'DEBUG: Max token l√†: 1536')
            return 1536
        else:
            print(f'DEBUG: Max token l√†: 2048')
            return 2048
    

    async def extract_text(self, images: List[Image.Image]) -> str:
        """Extract text from a list of PIL Images"""
        try:
            PROMPT = """
                B·∫°n l√† m·ªôt h·ªá th·ªëng tr√≠ch xu·∫•t th√¥ng tin h·ªçc thu·∫≠t c√≥ ƒë·ªô ch√≠nh x√°c cao.  
                H√£y ƒë·ªçc k·ªπ n·ªôi dung trong ·∫£nh ƒë·∫ßu v√†o, bao g·ªìm m·ªçi d·∫°ng d·ªØ li·ªáu: vƒÉn b·∫£n, c√¥ng th·ª©c, h√¨nh ·∫£nh, b·∫£ng bi·ªÉu, ch√∫ th√≠ch, ti√™u ƒë·ªÅ, s·ªë hi·ªáu h√¨nh/table, v√† m·ªëi li√™n k·∫øt gi·ªØa ch√∫ng.

                Y√äU C·∫¶U:
                1. Tr√≠ch xu·∫•t l·∫°i nguy√™n v·∫πn to√†n b·ªô n·ªôi dung c√≥ trong ·∫£nh.
                2. Kh√¥ng di·ªÖn gi·∫£i, kh√¥ng suy ƒëo√°n, kh√¥ng th√™m th√¥ng tin ngo√†i ·∫£nh.
                3. Gi·ªØ ƒë√∫ng c·∫•u tr√∫c logic theo th·ª© t·ª± xu·∫•t hi·ªán: ti√™u ƒë·ªÅ ‚Üí ƒëo·∫°n vƒÉn ‚Üí h√¨nh ‚Üí ch√∫ th√≠ch ‚Üí b·∫£ng ‚Üí ghi ch√∫.
                4. N·∫øu c√≥ h√¨nh/figure/table, ch·ªâ m√¥ t·∫£ l·∫°i b·∫±ng nh·ªØng g√¨ hi·ªÉn th·ªã k√®m ch√∫ th√≠ch (n·∫øu c√≥).
                5. Gi·ªØ nguy√™n c√°ch vi·∫øt: k√Ω t·ª±, d·∫•u, c√¥ng th·ª©c, s·ªë li·ªáu.
                6. ƒê·∫£m b·∫£o ƒë·∫ßu ra l√† m·ªôt ƒëo·∫°n vƒÉn ho√†n ch·ªânh, r√µ r√†ng v√† trung th·ª±c v·ªõi n·ªôi dung trong ·∫£nh.

                B·∫ÆT ƒê·∫¶U TR√çCH XU·∫§T D·ªÆ LI·ªÜU:
                """
            
            # Create messages for the model
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": PROMPT,
                        }
                    ]
                    + [{"type": "image", "image": img} for img in images],
                }
            ]

            # Prepare inputs
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            image_inputs, video_inputs = process_vision_info(messages)

            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to(self.device)

            max_tokens = max(self.estimate_output_tokens(img) for img in images)

            # Generate response
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs, 
                    max_new_tokens=max_tokens, 
                    temperature=0.1, 
                    do_sample=False,
                    use_cache=True,
                )

            # Decode response
            generated_ids_trimmed = [
                out_ids[len(in_ids) :]
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]

            output_text = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False,
            )[0]

            return output_text.strip()
            # return "Trong b√†i vi·∫øt n√†y m√¨nh xin gi·ªõi thi·ªáu ƒë·∫øn c√°c b·∫°n nh·ªØng l·ªánh c∆° b·∫£n nh·∫•t (C·∫ßn nh·ªõ v√† c√°ch ghi nh·ªõ n√≥) khi s·ª≠ d·ª•ng, l√†m vi·ªác v·ªõi Git. ƒê·ªìng th·ªùi m√¥ ph·ªèng m·ªôt quy tr√¨nh l√†m vi·ªác v·ªõi Git theo t·ª´ng giai ƒëo·∫°n. ·ªû giai ƒëo·∫°n n√†o th√¨ s·ª≠ d·ª•ng nh·ªØng l·ªánh n√†o. Nh·∫±m gi√∫p c√°c b·∫°n m·ªõi n·∫Øm ƒë∆∞·ª£c b·ª©c tranh t·ªïng quan nh·∫•t khi l√†m vi·ªác v·ªõi Git, ƒë·ªÉ s·ª≠ d·ª•ng d√≤ng l·ªánh cho ph√π h∆°p trong qu√° tr√¨nh h·ªçc t·∫≠p, th·ª±c h√†nh c≈©ng nh∆∞ ph·ª•c v·ª• c√¥ng vi·ªác sau n√†y.".strip()

        except Exception as e:
            logger.error(f"Error in text extraction: {e}")
            raise e
