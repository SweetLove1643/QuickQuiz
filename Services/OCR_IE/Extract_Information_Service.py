import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
from typing import List

torch.manual_seed(1234)

def get_device():
    if torch.cuda.is_available():
        print("üöÄ GPU CUDA detected ‚Üí using GPU")
        return "cuda"
    print("üñ• Running on CPU")
    return "cpu"

DEVICE = get_device()
DTYPE = torch.float16 if DEVICE == "cuda" else torch.float32

MODEL_ID = "Qwen/Qwen2-VL-2B-Instruct"
print("üîÑ Loading Qwen2-VL model... Please wait.")

try:
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        MODEL_ID,
        torch_dtype="auto",
        device_map="auto"
    )

    model.eval()
except Exception as e:
    print("[ERROR] Error while loading model:", repr(e), flush=True)
    raise

try:
    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
except Exception as e:
    print("[ERROR] Error while loading processor:", repr(e), flush=True)
    raise

print("‚úÖ Model & Processor loaded successfully!", flush=True)

PROMPT = """
B·∫°n l√† m·ªôt h·ªá th·ªëng tr√≠ch xu·∫•t th√¥ng tin h·ªçc thu·∫≠t c√≥ ƒë·ªô ch√≠nh x√°c cao.  
H√£y ƒë·ªçc k·ªπ n·ªôi dung trong ·∫£nh ƒë·∫ßu v√†o, bao g·ªìm m·ªçi d·∫°ng d·ªØ li·ªáu: vƒÉn b·∫£n, c√¥ng th·ª©c, h√¨nh ·∫£nh, b·∫£ng bi·ªÉu, ch√∫ th√≠ch, ti√™u ƒë·ªÅ, s·ªë hi·ªáu h√¨nh/table, v√† m·ªëi li√™n k·∫øt gi·ªØa ch√∫ng.

Y√äU C·∫¶U:
1. Tr√≠ch xu·∫•t l·∫°i nguy√™n v·∫πn to√†n b·ªô n·ªôi dung c√≥ trong ·∫£nh.
2. Kh√¥ng di·ªÖn gi·∫£i, kh√¥ng suy ƒëo√°n, kh√¥ng th√™m th√¥ng tin ngo√†i ·∫£nh.
3. Gi·ªØ ƒë√∫ng c·∫•u tr√∫c logic theo th·ª© t·ª± xu·∫•t hi·ªán: ti√™u ƒë·ªÅ ‚Üí ƒëo·∫°n vƒÉn ‚Üí h√¨nh ‚Üí ch√∫ th√≠ch ‚Üí b·∫£ng ‚Üí ghi ch√∫.
4. N·∫øu c√≥ h√¨nh/figure/table, ch·ªâ m√¥ t·∫£ l·∫°i b·∫±ng nh·ªØng g√¨ hi·ªÉn th·ªã k√®m ch√∫ th√≠ch (n·∫øu c√≥).
5. Gi·ªØ nguy√™n c√°ch vi·∫øt: k√Ω t·ª±, d·∫•u, c√¥ng th·ª©c, s·ªë li·ªáu.
6. ƒê·∫£m b·∫£o ƒë·∫ßu ra l√† m·ªôt ƒëo·∫°n vƒÉn ho√†n ch·ªânh, r√µ r√†ng v√† trung th·ª±c v·ªõi n·ªôi dung trong ·∫£nh.

B·∫ÆT ƒê·∫¶U TR√çCH XU·∫§T D·ªÆ LI·ªÜU:
"""

def estimate_output_tokens(image):
    width, height = image.size
    print(f'DEBUG: K√≠ch th∆∞·ªõc h√¨nh ·∫£nh ƒë·∫ßu v√†o l√†: {width}x{height}')
    num_pixels = width * height
    if num_pixels <= 512 * 512:
        print(f'DEBUG: Max token l√†: 256')
        return 256
    elif num_pixels <= 720 * 720:
        print(f'DEBUG: Max token l√†: 512')
        return 512
    elif num_pixels <= 1024 * 1024:
        print(f'DEBUG: Max token l√†: 1024')
        return 1024
    elif num_pixels <= 1536 * 1536:
        print(f'DEBUG: Max token l√†: 1536')
        return 1536
    else:
        print(f'DEBUG: Max token l√†: 2048')
        return 2048

def extract_information(images: List[Image.Image]) -> str:

    contents = [{"type": "image", "image": img} for img in images]
    contents.append({"type": "text", "text": PROMPT})

    messages = [
        {
            "role": "user",
            "content": contents
        }
    ]

    try:
        text = processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
    except Exception as e:
        print("[ERROR] Error in apply_chat_template:", repr(e), flush=True)
        raise

    image_inputs, video_inputs = process_vision_info(messages)
       
    try:

        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )

        inputs = inputs.to(DEVICE)
    except Exception as e:
        print("[ERROR] Error in processor(text, images):", repr(e), flush=True)
        raise

    print("[DEBUG] Calling model.generate...", flush=True)

    max_tokens = max(estimate_output_tokens(img) for img in images)

    try:
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=max_tokens, 
                use_cache=True,
            )
    except Exception as e:
        print("[ERROR] Error in model.generate:", repr(e), flush=True)
        raise


    try:
        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
    except Exception as e:
        print("[ERROR] Error in batch_decode:", repr(e), flush=True)
        raise

    return output_text






