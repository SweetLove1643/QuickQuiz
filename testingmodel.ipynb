{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c281ee6",
   "metadata": {},
   "source": [
    "# đ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d81879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\paddle\\utils\\cpp_extension\\extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\Admin\\.paddlex\\official_models\\PP-OCRv5_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('latin_PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\Admin\\.paddlex\\official_models\\latin_PP-OCRv5_mobile_rec`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_path': 'doc1.png', 'page_index': None, 'doc_preprocessor_res': {'input_path': None, 'page_index': None, 'input_img': array([[[25, ..., 11],\n",
      "        ...,\n",
      "        [25, ..., 11]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[25, ..., 11],\n",
      "        ...,\n",
      "        [25, ..., 11]]], shape=(236, 1052, 3), dtype=uint8), 'model_settings': {'use_doc_orientation_classify': False, 'use_doc_unwarping': False}, 'angle': -1, 'rot_img': array([[[25, ..., 11],\n",
      "        ...,\n",
      "        [25, ..., 11]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[25, ..., 11],\n",
      "        ...,\n",
      "        [25, ..., 11]]], shape=(236, 1052, 3), dtype=uint8), 'output_img': array([[[25, ..., 11],\n",
      "        ...,\n",
      "        [25, ..., 11]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[25, ..., 11],\n",
      "        ...,\n",
      "        [25, ..., 11]]], shape=(236, 1052, 3), dtype=uint8)}, 'dt_polys': [array([[17, 21],\n",
      "       ...,\n",
      "       [17, 46]], shape=(4, 2), dtype=int16), array([[ 18,  87],\n",
      "       ...,\n",
      "       [ 18, 109]], shape=(4, 2), dtype=int16), array([[ 18, 124],\n",
      "       ...,\n",
      "       [ 18, 145]], shape=(4, 2), dtype=int16), array([[ 18, 188],\n",
      "       ...,\n",
      "       [ 18, 209]], shape=(4, 2), dtype=int16)], 'model_settings': {'use_doc_preprocessor': True, 'use_textline_orientation': False}, 'text_det_params': {'limit_side_len': 64, 'limit_type': 'min', 'thresh': 0.3, 'max_side_limit': 4000, 'box_thresh': 0.6, 'unclip_ratio': 1.5}, 'text_type': 'general', 'text_rec_score_thresh': 0.0, 'return_word_box': False, 'rec_texts': ['Quantization', 'Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to', 'the Quantization overview for more available quantization backends.', 'The example below uses bitsandbytes to quantize the weights to 8-bits.'], 'rec_scores': [0.9999906420707703, 0.9892130494117737, 0.9959542751312256, 0.9916439652442932], 'rec_polys': [array([[17, 21],\n",
      "       ...,\n",
      "       [17, 46]], shape=(4, 2), dtype=int16), array([[ 18,  87],\n",
      "       ...,\n",
      "       [ 18, 109]], shape=(4, 2), dtype=int16), array([[ 18, 124],\n",
      "       ...,\n",
      "       [ 18, 145]], shape=(4, 2), dtype=int16), array([[ 18, 188],\n",
      "       ...,\n",
      "       [ 18, 209]], shape=(4, 2), dtype=int16)], 'vis_fonts': [<paddlex.utils.fonts.Font object at 0x0000017D7F9BCC10>, <paddlex.utils.fonts.Font object at 0x0000017D7F9BCC10>, <paddlex.utils.fonts.Font object at 0x0000017D7F9BCC10>, <paddlex.utils.fonts.Font object at 0x0000017D7F9BCC10>], 'textline_orientation_angles': [-1, -1, -1, -1], 'rec_boxes': array([[ 17, ...,  46],\n",
      "       ...,\n",
      "       [ 18, ..., 209]], shape=(4, 4), dtype=int16)}\n"
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "\n",
    "# Khởi tạo OCR hỗ trợ tiếng Việt\n",
    "ocr = PaddleOCR(\n",
    "    lang='vi',                     # Quan trọng: bật tiếng Việt\n",
    "    use_doc_orientation_classify=False,\n",
    "    use_doc_unwarping=False,\n",
    "    use_textline_orientation=False\n",
    ")\n",
    "\n",
    "# Ảnh đầu vào\n",
    "img_path = \"doc1.png\"\n",
    "\n",
    "# Nhận dạng chữ\n",
    "result = ocr.predict(img_path)\n",
    "\n",
    "for line in result:\n",
    "    print(line)\n",
    "    line.save_to_json(\"output\")  # Lưu từng dòng ra file JSON\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e68c13ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[ 17 ...  46]\n",
      " ...\n",
      " [ 18 ... 209]]\n",
      "=== In từng box ===\n",
      "17\n",
      "21\n",
      "158\n",
      "46\n",
      "18\n",
      "87\n",
      "1021\n",
      "109\n",
      "18\n",
      "124\n",
      "618\n",
      "145\n",
      "18\n",
      "188\n",
      "638\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "print(type(result[0]['rec_boxes']))\n",
    "print(result[0]['rec_boxes'])\n",
    "print(\"=== In từng box ===\")\n",
    "for box in result[0]['rec_boxes']:\n",
    "    for point in box:\n",
    "        print(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Căn c Quyt đnh s 2815/QĐ-ĐHSPKT ngày 13/9/2022 ca Hiu\n",
      "trưng trưng Đi hc Sư phm K thut Tp. HCM v vic ban hành quy\n",
      "đnh v Công tác Khen thưng và K lut đ vi sinh viên đi hc h chính\n",
      "quy ca Trưng Đi hc Sư phm K thut Thành ph H Chí Minh; Quyt\n",
      "đnh s 3289/QĐ-ĐHSPKT ngày 03/11/2022; Quyt đnh s 936/QĐ-\n",
      "ĐHSPKT ngày 31/3/2023, Quyt đnh s 2192/QĐ-ĐHSPKT ngày\n",
      "17/7/2023 và Quyt đnh s 468/QĐ-ĐHSPKT ngày 21/02/2024 Vê vic\n",
      "sa đi b sung mt s đu ca Quy đnh v Công tác Khen thưng và K\n",
      "lut đi v sinh viên đi hc h chính quy ca Trưng Đi hc Sư phm K\n",
      "thut Thành ph H Chí Minh;\n"
     ]
    }
   ],
   "source": [
    "paragrap = []\n",
    "for line in result:  \n",
    "    for sentence in line['rec_texts']:\n",
    "        paragrap.append(sentence)\n",
    "    line.save_to_json(\"output\")\n",
    "    \n",
    "\n",
    "for txt in paragrap:\n",
    "    print(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af84e395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9df2cce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR Output:\n",
      "EXCLUDING ON BACK:\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Load pretrained TrOCR\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "\n",
    "# Mở ảnh\n",
    "image_path = \"doc1.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# OCR\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"OCR Output:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb6f7c",
   "metadata": {},
   "source": [
    "# PaddleOCR + TrOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cac2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\Admin\\.paddlex\\official_models\\PP-LCNet_x1_0_doc_ori`.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\Admin\\.paddlex\\official_models\\UVDoc`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\Admin\\.paddlex\\official_models\\PP-LCNet_x1_0_textline_ori`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\Admin\\.paddlex\\official_models\\PP-OCRv5_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('latin_PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\Admin\\.paddlex\\official_models\\latin_PP-OCRv5_mobile_rec`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load image processor for './quickquiz/Models/checkp'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './quickquiz/Models/checkp' is the correct path to a directory containing a preprocessor_config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\image_processing_base.py:354\u001b[39m, in \u001b[36mImageProcessingMixin.get_image_processor_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    333\u001b[39m     resolved_image_processor_files = [\n\u001b[32m    334\u001b[39m         resolved_file\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m [image_processor_file, PROCESSOR_NAME]\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    353\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     resolved_image_processor_file = \u001b[43mresolved_image_processor_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m    356\u001b[39m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDevice:\u001b[39m\u001b[33m\"\u001b[39m, device)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# ✅ Load TrOCR – thêm trust_remote_code nếu version mới\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m processor = \u001b[43mTrOCRProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m model = VisionEncoderDecoderModel.from_pretrained(\n\u001b[32m     25\u001b[39m     MODEL_PATH,\n\u001b[32m     26\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     27\u001b[39m ).to(device)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ TrOCR loaded thành công\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\processing_utils.py:1394\u001b[39m, in \u001b[36mProcessorMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1392\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m] = token\n\u001b[32m-> \u001b[39m\u001b[32m1394\u001b[39m args = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1395\u001b[39m processor_dict, kwargs = \u001b[38;5;28mcls\u001b[39m.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_args_and_dict(args, processor_dict, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\processing_utils.py:1453\u001b[39m, in \u001b[36mProcessorMixin._get_arguments_from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1451\u001b[39m         attribute_class = \u001b[38;5;28mcls\u001b[39m.get_possibly_dynamic_module(class_name)\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m     args.append(\u001b[43mattribute_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1455\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:494\u001b[39m, in \u001b[36mAutoImageProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;66;03m# In case we have a config_dict, but it's not a timm config dict, we raise the initial exception,\u001b[39;00m\n\u001b[32m    492\u001b[39m     \u001b[38;5;66;03m# because only timm models have image processing in `config.json`.\u001b[39;00m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_timm_config_dict(config_dict):\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m initial_exception\n\u001b[32m    496\u001b[39m image_processor_type = config_dict.get(\u001b[33m\"\u001b[39m\u001b[33mimage_processor_type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    497\u001b[39m image_processor_auto_map = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:476\u001b[39m, in \u001b[36mAutoImageProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# Load the image processor config\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# Main path for all transformers models and local TimmWrapper checkpoints\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     config_dict, _ = \u001b[43mImageProcessingMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_processor_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m initial_exception:\n\u001b[32m    480\u001b[39m     \u001b[38;5;66;03m# Fallback path for Hub TimmWrapper checkpoints. Timm models' image processing is saved in `config.json`\u001b[39;00m\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# instead of `preprocessor_config.json`. Because this is an Auto class and we don't have any information\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# except the model name, the only way to check if a remote checkpoint is a timm model is to try to\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;66;03m# load `config.json` and if it fails with some error, we raise the initial exception.\u001b[39;00m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\image_processing_base.py:361\u001b[39m, in \u001b[36mImageProcessingMixin.get_image_processor_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    360\u001b[39m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    362\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt load image processor for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. If you were trying to load\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    363\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m it from \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, make sure you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a local directory with the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    364\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m same name. Otherwise, make sure \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is the correct path to a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    365\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m directory containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_processor_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m         )\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    369\u001b[39m     \u001b[38;5;66;03m# Load image_processor dict\u001b[39;00m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(resolved_image_processor_file, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n",
      "\u001b[31mOSError\u001b[39m: Can't load image processor for './quickquiz/Models/checkp'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './quickquiz/Models/checkp' is the correct path to a directory containing a preprocessor_config.json file"
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# ✅ PaddleOCR chỉ dùng để detect bbox\n",
    "paddleocr = PaddleOCR(lang=\"vi\")\n",
    "\n",
    "# ✅ Model TrOCR dành cho text in\n",
    "TROCR_MODEL = \"microsoft/trocr-base-printed\"\n",
    "MODEL_PATH = \"./quickquiz/Models/checkp\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ✅ Load TrOCR – thêm trust_remote_code nếu version mới\n",
    "processor = TrOCRProcessor.from_pretrained(\n",
    "    TROCR_MODEL,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "print(\"✅ TrOCR loaded thành công\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21cdb712",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load image processor for './quickquiz/Models/checkp'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './quickquiz/Models/checkp' is the correct path to a directory containing a preprocessor_config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\image_processing_base.py:354\u001b[39m, in \u001b[36mImageProcessingMixin.get_image_processor_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    333\u001b[39m     resolved_image_processor_files = [\n\u001b[32m    334\u001b[39m         resolved_file\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m [image_processor_file, PROCESSOR_NAME]\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    353\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     resolved_image_processor_file = \u001b[43mresolved_image_processor_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m    356\u001b[39m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrOCRProcessor, VisionEncoderDecoderModel\n\u001b[32m      5\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m processor = \u001b[43mTrOCRProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m model = VisionEncoderDecoderModel.from_pretrained(MODEL_PATH).to(device)\n\u001b[32m      9\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\processing_utils.py:1394\u001b[39m, in \u001b[36mProcessorMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1392\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m] = token\n\u001b[32m-> \u001b[39m\u001b[32m1394\u001b[39m args = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1395\u001b[39m processor_dict, kwargs = \u001b[38;5;28mcls\u001b[39m.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_args_and_dict(args, processor_dict, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\processing_utils.py:1453\u001b[39m, in \u001b[36mProcessorMixin._get_arguments_from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1451\u001b[39m         attribute_class = \u001b[38;5;28mcls\u001b[39m.get_possibly_dynamic_module(class_name)\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m     args.append(\u001b[43mattribute_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1455\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:494\u001b[39m, in \u001b[36mAutoImageProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;66;03m# In case we have a config_dict, but it's not a timm config dict, we raise the initial exception,\u001b[39;00m\n\u001b[32m    492\u001b[39m     \u001b[38;5;66;03m# because only timm models have image processing in `config.json`.\u001b[39;00m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_timm_config_dict(config_dict):\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m initial_exception\n\u001b[32m    496\u001b[39m image_processor_type = config_dict.get(\u001b[33m\"\u001b[39m\u001b[33mimage_processor_type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    497\u001b[39m image_processor_auto_map = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:476\u001b[39m, in \u001b[36mAutoImageProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# Load the image processor config\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# Main path for all transformers models and local TimmWrapper checkpoints\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     config_dict, _ = \u001b[43mImageProcessingMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_processor_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m initial_exception:\n\u001b[32m    480\u001b[39m     \u001b[38;5;66;03m# Fallback path for Hub TimmWrapper checkpoints. Timm models' image processing is saved in `config.json`\u001b[39;00m\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# instead of `preprocessor_config.json`. Because this is an Auto class and we don't have any information\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# except the model name, the only way to check if a remote checkpoint is a timm model is to try to\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;66;03m# load `config.json` and if it fails with some error, we raise the initial exception.\u001b[39;00m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\image_processing_base.py:361\u001b[39m, in \u001b[36mImageProcessingMixin.get_image_processor_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    360\u001b[39m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    362\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt load image processor for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. If you were trying to load\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    363\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m it from \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, make sure you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a local directory with the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    364\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m same name. Otherwise, make sure \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is the correct path to a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    365\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m directory containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_processor_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m         )\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    369\u001b[39m     \u001b[38;5;66;03m# Load image_processor dict\u001b[39;00m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(resolved_image_processor_file, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n",
      "\u001b[31mOSError\u001b[39m: Can't load image processor for './quickquiz/Models/checkp'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './quickquiz/Models/checkp' is the correct path to a directory containing a preprocessor_config.json file"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(MODEL_PATH)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_PATH).to(device)\n",
    "model.eval()\n",
    "\n",
    "# load ảnh thô\n",
    "image = Image.open(\"test.jpg\").convert(\"RGB\")\n",
    "\n",
    "# xử lý đúng chuẩn\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs)\n",
    "\n",
    "text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"Kết quả:\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n"
     ]
    }
   ],
   "source": [
    "img_path = \"trocr.png\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "input = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "generated_ids = model.generate(input)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d56d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_text_boxes(image_path):\n",
    "    \"\"\"\n",
    "    Nhận đầu vào: path ảnh\n",
    "    Trả về: list bbox dạng [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "\n",
    "    # Chạy detect\n",
    "    result = paddleocr.predict(image_path)\n",
    "\n",
    "    bboxes = []\n",
    "\n",
    "    for box in result[0]['rec_boxes']:\n",
    "        bboxes.append(box)\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101d46c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số vùng phát hiện: 10\n",
      "[54 ... 14]\n",
      "[ 0 ... 43]\n",
      "[ 0 ... 70]\n",
      "[ 0 ... 97]\n",
      "[  0 ... 122]\n",
      "[  0 ... 149]\n",
      "[  0 ... 175]\n",
      "[  0 ... 200]\n",
      "[  0 ... 227]\n",
      "[  0 ... 253]\n"
     ]
    }
   ],
   "source": [
    "image_path = \"doc.png\"  # đổi thành file bạn có\n",
    "bboxes = detect_text_boxes(image_path)\n",
    "\n",
    "print(\"Số vùng phát hiện:\", len(bboxes))\n",
    "for b in bboxes:\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5200abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def crop_boxes(image_path, bboxes):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        image_path: đường dẫn ảnh gốc\n",
    "        bboxes: list bbox dạng [x1, y1, x2, y2]\n",
    "    Output:\n",
    "        list các ảnh crop (PIL.Image)\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    crops = []\n",
    "\n",
    "    for (x1, y1, x2, y2) in bboxes:\n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "        crops.append(crop)\n",
    "\n",
    "    return crops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9753154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số ảnh crop: 10\n"
     ]
    }
   ],
   "source": [
    "crops = crop_boxes(image_path, bboxes)\n",
    "\n",
    "print(\"Số ảnh crop:\", len(crops))\n",
    "crops[4].show()   # hiển thị crop dòng đầu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423678ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_case(text):\n",
    "    text = text.lower()  # toàn bộ lower\n",
    "    text = re.sub(r'([.!?]\\s+)([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)  # viết hoa đầu câu\n",
    "    if text: text = text[0].upper() + text[1:]\n",
    "    return text\n",
    "\n",
    "def trocr_read(image):\n",
    "    # image: PIL.Image crop\n",
    "    \n",
    "    inputs = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=256,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    print(\"=== TrOCR output:\", text)\n",
    "    return normalize_case(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6392d70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TrOCR output: \n",
      "=== TrOCR output: \n",
      "=== TrOCR output: \n",
      "=== TrOCR output: \n",
      "=== TrOCR output: \n",
      "=== TrOCR output: \n",
      "=== TrOCR output: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m texts = \u001b[43m[\u001b[49m\u001b[43mtrocr_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcrops\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m texts = [\u001b[43mtrocr_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m crops]\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrocr_read\u001b[39m\u001b[34m(image)\u001b[39m\n\u001b[32m     10\u001b[39m inputs = processor(image, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).pixel_values.to(device)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m text = processor.batch_decode(generated_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m].strip()\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== TrOCR output:\u001b[39m\u001b[33m\"\u001b[39m, text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3265\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3262\u001b[39m flat_running_sequences = \u001b[38;5;28mself\u001b[39m._flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[32m   3263\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(flat_running_sequences, **model_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m3265\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3267\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3268\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3269\u001b[39m     model_outputs,\n\u001b[32m   3270\u001b[39m     model_kwargs,\n\u001b[32m   3271\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3272\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py:553\u001b[39m, in \u001b[36mVisionEncoderDecoderModel.forward\u001b[39m\u001b[34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m     decoder_input_ids = shift_tokens_right(\n\u001b[32m    549\u001b[39m         labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m    550\u001b[39m     )\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m# Compute loss independent from decoder (as some shift the logits inside them)\u001b[39;00m\n\u001b[32m    569\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\models\\trocr\\modeling_trocr.py:844\u001b[39m, in \u001b[36mTrOCRForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    827\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m    828\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model.decoder(\n\u001b[32m    829\u001b[39m     input_ids=input_ids,\n\u001b[32m    830\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    841\u001b[39m     cache_position=cache_position,\n\u001b[32m    842\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    846\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "texts = [trocr_read(c) for c in crops]\n",
    "\n",
    "for t in texts:\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab8baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OCR Paragraph =====\n",
      "\n",
      "Cashier\n",
      "Please with receipt notebook\n",
      "Thanks condited when the primo well not refund with usen follow bebook\n",
      "Qty cla triding pai hoc stornam ki thuat thank dh6 h6 chi minn: ovey\n",
      "Dinh s6 3289/qb-bhspkt nav 03/11/2022; qtyet dinh s6 936/qb-\n",
      "Bhspkt agay 31/3/2023, quyet dinh s6 2192/qb-bhspkt regay\n",
      "17/7/2023 va quyet dinh s6 468/qb-bhspkt regay 21/02/2024 ve viec\n",
      "Sube doi do sing not so diew cua qty dinnive cong tac khen thurong ya ky\n",
      "Please for non when of recipe chown qty cla invoing be inoc summary\n",
      "Inual inamn pro ho chi minn,\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\\n\".join(texts)\n",
    "print(\"\\n===== OCR Paragraph =====\\n\")\n",
    "print(paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7121a",
   "metadata": {},
   "source": [
    "# Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc7fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572154c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ==== Cấu hình cơ bản ====\n",
    "BASE_MODEL = \"VietAI/vit5-base\"\n",
    "LORA_PATH = \"./quickquiz/Models/ViT5_checkpoint_epochs4\"\n",
    "\n",
    "# Seed\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b163ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)\n",
    "model = PeftModel.from_pretrained(base_model, LORA_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdc4efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    \"\"\"\n",
    "    Sử dụng model summaries để tóm tắt văn bản.\n",
    "    Model: mT5, ViT5,...\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        f'Tóm tắt: {text}',\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=250,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faaa64be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker là một nền tảng phần mềm giúp bạn building, deploying và running ứng dụng dễ dàng hơn bằng cách sử dụng các container tiêu chuẩn hóa. \n"
     ]
    }
   ],
   "source": [
    "text = '''Docker là một nền tảng phần mềm giúp bạn building, deploying và running ứng dụng dễ dàng hơn bằng cách sử dụng các containers (trên nền tảng ảo hóa).\n",
    "Docker đóng gói phần mềm thành các container tiêu chuẩn hóa, chứa đựng tất cả những thứ cần thiết để phần mềm hoạt động như thư viện, công cụ hệ thống, mã nguồn và thời gian chạy. Khi cần deploy app lên bất kỳ server nào, bạn chỉ cần run container của Docker thì app của bạn sẽ được khởi chạy ngay lập tức.\n",
    "\n",
    "Khi sử dụng Docker, bạn có thể dễ dàng triển khai và mở rộng quy mô ứng dụng trong bất kỳ môi trường nào, đồng thời đảm bảo rằng mã nguồn của bạn sẽ luôn chạy được một cách ổn định.\n",
    "Container là các gói phần mềm nhỏ gọn chứa tất cả các thành phần cần thiết của một ứng dụng như mã nguồn, thư viện, và các công cụ, giúp đảm bảo ứng dụng có thể chạy đồng nhất trên mọi môi trường.\n",
    "\n",
    "Bằng cách đó, nhờ vào container, ứng dụng sẽ chạy trên mọi máy Linux khác bất kể mọi cài đặt tùy chỉnh mà máy có thể khác với máy được sử dụng để viết code.\n",
    "'''\n",
    "print(summarize_text(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f41890",
   "metadata": {},
   "source": [
    "# DeepSeek OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8eb5de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LlamaFlashAttention2' from 'transformers.models.llama.modeling_llama' (d:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33mdeepseek-ai/DeepSeek-OCR\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      7\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m model = model.eval().cuda().to(torch.bfloat16)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# prompt = \"<image>\\nFree OCR. \"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:549\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    547\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1346\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1341\u001b[39m     trust_remote_code = resolve_trust_remote_code(\n\u001b[32m   1342\u001b[39m         trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n\u001b[32m   1343\u001b[39m     )\n\u001b[32m   1345\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m-> \u001b[39m\u001b[32m1346\u001b[39m     config_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1349\u001b[39m     config_class.register_for_auto_class()\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:616\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m    604\u001b[39m final_module = get_cached_module_file(\n\u001b[32m    605\u001b[39m     repo_id,\n\u001b[32m    606\u001b[39m     module_file + \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    614\u001b[39m     repo_type=repo_type,\n\u001b[32m    615\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:311\u001b[39m, in \u001b[36mget_class_in_module\u001b[39m\u001b[34m(class_name, module_path, force_reload)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# reload in both cases, unless the module is already imported and the hash hits\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33m__transformers_module_hash__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) != module_hash:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[43mmodule_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     module.__transformers_module_hash__ = module_hash\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\transformers_modules\\deepseek_hyphen_ai\\DeepSeek_hyphen_OCR\\2c968b433af61a059311cbf8997765023806a24d\\modeling_deepseekocr.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_deepseekv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepseekV2Model, DeepseekV2ForCausalLM\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_deepseek_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepseekV2Config\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModelOutputWithPast, CausalLMOutputWithPast\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\transformers_modules\\deepseek_hyphen_ai\\DeepSeek_hyphen_OCR\\2c968b433af61a059311cbf8997765023806a24d\\modeling_deepseekv2.py:37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_attn_mask_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _prepare_4d_causal_attention_mask\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_llama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     38\u001b[39m     LlamaAttention,\n\u001b[32m     39\u001b[39m     LlamaFlashAttention2\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     42\u001b[39m     BaseModelOutputWithPast,\n\u001b[32m     43\u001b[39m     CausalLMOutputWithPast,\n\u001b[32m     44\u001b[39m     SequenceClassifierOutputWithPast,\n\u001b[32m     45\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'LlamaFlashAttention2' from 'transformers.models.llama.modeling_llama' (d:\\Study\\Projects\\HK7\\QuichQuiz\\venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "model_name = 'deepseek-ai/DeepSeek-OCR'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name,trust_remote_code=True, use_safetensors=True)\n",
    "model = model.eval().cuda().to(torch.bfloat16)\n",
    "\n",
    "# prompt = \"<image>\\nFree OCR. \"\n",
    "prompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
    "image_file = 'doc1.png'\n",
    "output_path = 'your/output/dir'\n",
    "\n",
    "res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c691009",
   "metadata": {},
   "source": [
    "# TrOCR prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebac98bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XỬ LÝ TẬP: TRAIN =====\n",
      "Tổng số mẫu: 280897\n",
      "Tỉ lệ: 3.0% -> lấy 8426\n",
      "Đã ghi labels mới vào: data_small\\train\\labels.txt\n",
      "  + Copy 50/8426\n",
      "  + Copy 100/8426\n",
      "  + Copy 150/8426\n",
      "  + Copy 200/8426\n",
      "  + Copy 250/8426\n",
      "  + Copy 300/8426\n",
      "  + Copy 350/8426\n",
      "  + Copy 400/8426\n",
      "  + Copy 450/8426\n",
      "  + Copy 500/8426\n",
      "  + Copy 550/8426\n",
      "  + Copy 600/8426\n",
      "  + Copy 650/8426\n",
      "  + Copy 700/8426\n",
      "  + Copy 750/8426\n",
      "  + Copy 800/8426\n",
      "  + Copy 850/8426\n",
      "  + Copy 900/8426\n",
      "  + Copy 950/8426\n",
      "  + Copy 1000/8426\n",
      "  + Copy 1050/8426\n",
      "  + Copy 1100/8426\n",
      "  + Copy 1150/8426\n",
      "  + Copy 1200/8426\n",
      "  + Copy 1250/8426\n",
      "  + Copy 1300/8426\n",
      "  + Copy 1350/8426\n",
      "  + Copy 1400/8426\n",
      "  + Copy 1450/8426\n",
      "  + Copy 1500/8426\n",
      "  + Copy 1550/8426\n",
      "  + Copy 1600/8426\n",
      "  + Copy 1650/8426\n",
      "  + Copy 1700/8426\n",
      "  + Copy 1750/8426\n",
      "  + Copy 1800/8426\n",
      "  + Copy 1850/8426\n",
      "  + Copy 1900/8426\n",
      "  + Copy 1950/8426\n",
      "  + Copy 2000/8426\n",
      "  + Copy 2050/8426\n",
      "  + Copy 2100/8426\n",
      "  + Copy 2150/8426\n",
      "  + Copy 2200/8426\n",
      "  + Copy 2250/8426\n",
      "  + Copy 2300/8426\n",
      "  + Copy 2350/8426\n",
      "  + Copy 2400/8426\n",
      "  + Copy 2450/8426\n",
      "  + Copy 2500/8426\n",
      "  + Copy 2550/8426\n",
      "  + Copy 2600/8426\n",
      "  + Copy 2650/8426\n",
      "  + Copy 2700/8426\n",
      "  + Copy 2750/8426\n",
      "  + Copy 2800/8426\n",
      "  + Copy 2850/8426\n",
      "  + Copy 2900/8426\n",
      "  + Copy 2950/8426\n",
      "  + Copy 3000/8426\n",
      "  + Copy 3050/8426\n",
      "  + Copy 3100/8426\n",
      "  + Copy 3150/8426\n",
      "  + Copy 3200/8426\n",
      "  + Copy 3250/8426\n",
      "  + Copy 3300/8426\n",
      "  + Copy 3350/8426\n",
      "  + Copy 3400/8426\n",
      "  + Copy 3450/8426\n",
      "  + Copy 3500/8426\n",
      "  + Copy 3550/8426\n",
      "  + Copy 3600/8426\n",
      "  + Copy 3650/8426\n",
      "  + Copy 3700/8426\n",
      "  + Copy 3750/8426\n",
      "  + Copy 3800/8426\n",
      "  + Copy 3850/8426\n",
      "  + Copy 3900/8426\n",
      "  + Copy 3950/8426\n",
      "  + Copy 4000/8426\n",
      "  + Copy 4050/8426\n",
      "  + Copy 4100/8426\n",
      "  + Copy 4150/8426\n",
      "  + Copy 4200/8426\n",
      "  + Copy 4250/8426\n",
      "  + Copy 4300/8426\n",
      "  + Copy 4350/8426\n",
      "  + Copy 4400/8426\n",
      "  + Copy 4450/8426\n",
      "  + Copy 4500/8426\n",
      "  + Copy 4550/8426\n",
      "  + Copy 4600/8426\n",
      "  + Copy 4650/8426\n",
      "  + Copy 4700/8426\n",
      "  + Copy 4750/8426\n",
      "  + Copy 4800/8426\n",
      "  + Copy 4850/8426\n",
      "  + Copy 4900/8426\n",
      "  + Copy 4950/8426\n",
      "  + Copy 5000/8426\n",
      "  + Copy 5050/8426\n",
      "  + Copy 5100/8426\n",
      "  + Copy 5150/8426\n",
      "  + Copy 5200/8426\n",
      "  + Copy 5250/8426\n",
      "  + Copy 5300/8426\n",
      "  + Copy 5350/8426\n",
      "  + Copy 5400/8426\n",
      "  + Copy 5450/8426\n",
      "  + Copy 5500/8426\n",
      "  + Copy 5550/8426\n",
      "  + Copy 5600/8426\n",
      "  + Copy 5650/8426\n",
      "  + Copy 5700/8426\n",
      "  + Copy 5750/8426\n",
      "  + Copy 5800/8426\n",
      "  + Copy 5850/8426\n",
      "  + Copy 5900/8426\n",
      "  + Copy 5950/8426\n",
      "  + Copy 6000/8426\n",
      "  + Copy 6050/8426\n",
      "  + Copy 6100/8426\n",
      "  + Copy 6150/8426\n",
      "  + Copy 6200/8426\n",
      "  + Copy 6250/8426\n",
      "  + Copy 6300/8426\n",
      "  + Copy 6350/8426\n",
      "  + Copy 6400/8426\n",
      "  + Copy 6450/8426\n",
      "  + Copy 6500/8426\n",
      "  + Copy 6550/8426\n",
      "  + Copy 6600/8426\n",
      "  + Copy 6650/8426\n",
      "  + Copy 6700/8426\n",
      "  + Copy 6750/8426\n",
      "  + Copy 6800/8426\n",
      "  + Copy 6850/8426\n",
      "  + Copy 6900/8426\n",
      "  + Copy 6950/8426\n",
      "  + Copy 7000/8426\n",
      "  + Copy 7050/8426\n",
      "  + Copy 7100/8426\n",
      "  + Copy 7150/8426\n",
      "  + Copy 7200/8426\n",
      "  + Copy 7250/8426\n",
      "  + Copy 7300/8426\n",
      "  + Copy 7350/8426\n",
      "  + Copy 7400/8426\n",
      "  + Copy 7450/8426\n",
      "  + Copy 7500/8426\n",
      "  + Copy 7550/8426\n",
      "  + Copy 7600/8426\n",
      "  + Copy 7650/8426\n",
      "  + Copy 7700/8426\n",
      "  + Copy 7750/8426\n",
      "  + Copy 7800/8426\n",
      "  + Copy 7850/8426\n",
      "  + Copy 7900/8426\n",
      "  + Copy 7950/8426\n",
      "  + Copy 8000/8426\n",
      "  + Copy 8050/8426\n",
      "  + Copy 8100/8426\n",
      "  + Copy 8150/8426\n",
      "  + Copy 8200/8426\n",
      "  + Copy 8250/8426\n",
      "  + Copy 8300/8426\n",
      "  + Copy 8350/8426\n",
      "  + Copy 8400/8426\n",
      "  + Copy 8426/8426\n",
      "Kết quả:\n",
      "  - Copy thành công: 8426\n",
      "  - Ảnh thiếu: 0\n",
      "  - Labels ghi: 8426\n",
      "\n",
      "===== XỬ LÝ TẬP: VALID =====\n",
      "Tổng số mẫu: 40128\n",
      "Tỉ lệ: 2.0% -> lấy 802\n",
      "Đã ghi labels mới vào: data_small\\valid\\labels.txt\n",
      "  + Copy 50/802\n",
      "  + Copy 100/802\n",
      "  + Copy 150/802\n",
      "  + Copy 200/802\n",
      "  + Copy 250/802\n",
      "  + Copy 300/802\n",
      "  + Copy 350/802\n",
      "  + Copy 400/802\n",
      "  + Copy 450/802\n",
      "  + Copy 500/802\n",
      "  + Copy 550/802\n",
      "  + Copy 600/802\n",
      "  + Copy 650/802\n",
      "  + Copy 700/802\n",
      "  + Copy 750/802\n",
      "  + Copy 800/802\n",
      "  + Copy 802/802\n",
      "Kết quả:\n",
      "  - Copy thành công: 802\n",
      "  - Ảnh thiếu: 0\n",
      "  - Labels ghi: 802\n",
      "\n",
      "===== XỬ LÝ TẬP: TEST =====\n",
      "Tổng số mẫu: 80257\n",
      "Tỉ lệ: 10.0% -> lấy 8025\n",
      "Đã ghi labels mới vào: data_small\\test\\labels.txt\n",
      "  + Copy 50/8025\n",
      "  + Copy 100/8025\n",
      "  + Copy 150/8025\n",
      "  + Copy 200/8025\n",
      "  + Copy 250/8025\n",
      "  + Copy 300/8025\n",
      "  + Copy 350/8025\n",
      "  + Copy 400/8025\n",
      "  + Copy 450/8025\n",
      "  + Copy 500/8025\n",
      "  + Copy 550/8025\n",
      "  + Copy 600/8025\n",
      "  + Copy 650/8025\n",
      "  + Copy 700/8025\n",
      "  + Copy 750/8025\n",
      "  + Copy 800/8025\n",
      "  + Copy 850/8025\n",
      "  + Copy 900/8025\n",
      "  + Copy 950/8025\n",
      "  + Copy 1000/8025\n",
      "  + Copy 1050/8025\n",
      "  + Copy 1100/8025\n",
      "  + Copy 1150/8025\n",
      "  + Copy 1200/8025\n",
      "  + Copy 1250/8025\n",
      "  + Copy 1300/8025\n",
      "  + Copy 1350/8025\n",
      "  + Copy 1400/8025\n",
      "  + Copy 1450/8025\n",
      "  + Copy 1500/8025\n",
      "  + Copy 1550/8025\n",
      "  + Copy 1600/8025\n",
      "  + Copy 1650/8025\n",
      "  + Copy 1700/8025\n",
      "  + Copy 1750/8025\n",
      "  + Copy 1800/8025\n",
      "  + Copy 1850/8025\n",
      "  + Copy 1900/8025\n",
      "  + Copy 1950/8025\n",
      "  + Copy 2000/8025\n",
      "  + Copy 2050/8025\n",
      "  + Copy 2100/8025\n",
      "  + Copy 2150/8025\n",
      "  + Copy 2200/8025\n",
      "  + Copy 2250/8025\n",
      "  + Copy 2300/8025\n",
      "  + Copy 2350/8025\n",
      "  + Copy 2400/8025\n",
      "  + Copy 2450/8025\n",
      "  + Copy 2500/8025\n",
      "  + Copy 2550/8025\n",
      "  + Copy 2600/8025\n",
      "  + Copy 2650/8025\n",
      "  + Copy 2700/8025\n",
      "  + Copy 2750/8025\n",
      "  + Copy 2800/8025\n",
      "  + Copy 2850/8025\n",
      "  + Copy 2900/8025\n",
      "  + Copy 2950/8025\n",
      "  + Copy 3000/8025\n",
      "  + Copy 3050/8025\n",
      "  + Copy 3100/8025\n",
      "  + Copy 3150/8025\n",
      "  + Copy 3200/8025\n",
      "  + Copy 3250/8025\n",
      "  + Copy 3300/8025\n",
      "  + Copy 3350/8025\n",
      "  + Copy 3400/8025\n",
      "  + Copy 3450/8025\n",
      "  + Copy 3500/8025\n",
      "  + Copy 3550/8025\n",
      "  + Copy 3600/8025\n",
      "  + Copy 3650/8025\n",
      "  + Copy 3700/8025\n",
      "  + Copy 3750/8025\n",
      "  + Copy 3800/8025\n",
      "  + Copy 3850/8025\n",
      "  + Copy 3900/8025\n",
      "  + Copy 3950/8025\n",
      "  + Copy 4000/8025\n",
      "  + Copy 4050/8025\n",
      "  + Copy 4100/8025\n",
      "  + Copy 4150/8025\n",
      "  + Copy 4200/8025\n",
      "  + Copy 4250/8025\n",
      "  + Copy 4300/8025\n",
      "  + Copy 4350/8025\n",
      "  + Copy 4400/8025\n",
      "  + Copy 4450/8025\n",
      "  + Copy 4500/8025\n",
      "  + Copy 4550/8025\n",
      "  + Copy 4600/8025\n",
      "  + Copy 4650/8025\n",
      "  + Copy 4700/8025\n",
      "  + Copy 4750/8025\n",
      "  + Copy 4800/8025\n",
      "  + Copy 4850/8025\n",
      "  + Copy 4900/8025\n",
      "  + Copy 4950/8025\n",
      "  + Copy 5000/8025\n",
      "  + Copy 5050/8025\n",
      "  + Copy 5100/8025\n",
      "  + Copy 5150/8025\n",
      "  + Copy 5200/8025\n",
      "  + Copy 5250/8025\n",
      "  + Copy 5300/8025\n",
      "  + Copy 5350/8025\n",
      "  + Copy 5400/8025\n",
      "  + Copy 5450/8025\n",
      "  + Copy 5500/8025\n",
      "  + Copy 5550/8025\n",
      "  + Copy 5600/8025\n",
      "  + Copy 5650/8025\n",
      "  + Copy 5700/8025\n",
      "  + Copy 5750/8025\n",
      "  + Copy 5800/8025\n",
      "  + Copy 5850/8025\n",
      "  + Copy 5900/8025\n",
      "  + Copy 5950/8025\n",
      "  + Copy 6000/8025\n",
      "  + Copy 6050/8025\n",
      "  + Copy 6100/8025\n",
      "  + Copy 6150/8025\n",
      "  + Copy 6200/8025\n",
      "  + Copy 6250/8025\n",
      "  + Copy 6300/8025\n",
      "  + Copy 6350/8025\n",
      "  + Copy 6400/8025\n",
      "  + Copy 6450/8025\n",
      "  + Copy 6500/8025\n",
      "  + Copy 6550/8025\n",
      "  + Copy 6600/8025\n",
      "  + Copy 6650/8025\n",
      "  + Copy 6700/8025\n",
      "  + Copy 6750/8025\n",
      "  + Copy 6800/8025\n",
      "  + Copy 6850/8025\n",
      "  + Copy 6900/8025\n",
      "  + Copy 6950/8025\n",
      "  + Copy 7000/8025\n",
      "  + Copy 7050/8025\n",
      "  + Copy 7100/8025\n",
      "  + Copy 7150/8025\n",
      "  + Copy 7200/8025\n",
      "  + Copy 7250/8025\n",
      "  + Copy 7300/8025\n",
      "  + Copy 7350/8025\n",
      "  + Copy 7400/8025\n",
      "  + Copy 7450/8025\n",
      "  + Copy 7500/8025\n",
      "  + Copy 7550/8025\n",
      "  + Copy 7600/8025\n",
      "  + Copy 7650/8025\n",
      "  + Copy 7700/8025\n",
      "  + Copy 7750/8025\n",
      "  + Copy 7800/8025\n",
      "  + Copy 7850/8025\n",
      "  + Copy 7900/8025\n",
      "  + Copy 7950/8025\n",
      "  + Copy 8000/8025\n",
      "  + Copy 8025/8025\n",
      "Kết quả:\n",
      "  - Copy thành công: 8025\n",
      "  - Ảnh thiếu: 0\n",
      "  - Labels ghi: 8025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def extract_split(src_root, dst_root, split, ratio):\n",
    "    print(f\"\\n===== XỬ LÝ TẬP: {split.upper()} =====\")\n",
    "\n",
    "    src_img_folder = os.path.join(src_root, split, \"images\")\n",
    "    src_label_file = os.path.join(src_root, split, \"labels.txt\")\n",
    "\n",
    "    dst_split_folder = os.path.join(dst_root, split)\n",
    "    dst_img_folder = os.path.join(dst_split_folder, \"images\")\n",
    "    dst_label_file = os.path.join(dst_split_folder, \"labels.txt\")\n",
    "\n",
    "    os.makedirs(dst_img_folder, exist_ok=True)\n",
    "\n",
    "    # Đọc labels\n",
    "    with open(src_label_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    total = len(lines)\n",
    "    sample_size = max(1, int(total * ratio))\n",
    "\n",
    "    print(f\"Tổng số mẫu: {total}\")\n",
    "    print(f\"Tỉ lệ: {ratio*100:.1f}% -> lấy {sample_size}\")\n",
    "\n",
    "    random.seed(42)\n",
    "    sampled = random.sample(lines, sample_size)\n",
    "\n",
    "    # Lưu labels nhỏ\n",
    "    with open(dst_label_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(sampled)\n",
    "\n",
    "    print(f\"Đã ghi labels mới vào: {dst_label_file}\")\n",
    "\n",
    "    # Copy ảnh phù hợp\n",
    "    missing = 0\n",
    "    copied = 0\n",
    "\n",
    "    for idx, line in enumerate(sampled):\n",
    "        # Format: images/xxx.jpg|text...\n",
    "        left, right = line.split(\"|\", 1)\n",
    "        img_name = left.strip()\n",
    "\n",
    "        # Bỏ \"images/\" ở đầu\n",
    "        if img_name.startswith(\"images/\"):\n",
    "            img_name = img_name.replace(\"images/\", \"\", 1)\n",
    "\n",
    "        src_img_path = os.path.join(src_img_folder, img_name)\n",
    "        dst_img_path = os.path.join(dst_img_folder, img_name)\n",
    "\n",
    "        if not os.path.exists(src_img_path):\n",
    "            print(f\"[CẢNH BÁO] Ảnh không tồn tại: {img_name}\")\n",
    "            missing += 1\n",
    "            continue\n",
    "\n",
    "        shutil.copy(src_img_path, dst_img_path)\n",
    "        copied += 1\n",
    "\n",
    "        if (idx + 1) % 50 == 0 or idx == sample_size - 1:\n",
    "            print(f\"  + Copy {idx+1}/{sample_size}\")\n",
    "\n",
    "    print(\"Kết quả:\")\n",
    "    print(f\"  - Copy thành công: {copied}\")\n",
    "    print(f\"  - Ảnh thiếu: {missing}\")\n",
    "    print(f\"  - Labels ghi: {sample_size}\")\n",
    "\n",
    "\n",
    "def extract_all(src_root, dst_root, ratios):\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        extract_split(src_root, dst_root, split, ratios.get(split, 0))\n",
    "\n",
    "\n",
    "# Tỉ lệ do bạn chọn\n",
    "ratios = {\n",
    "    \"train\": 0.03,\n",
    "    \"valid\": 0.02,\n",
    "    \"test\":  0.1\n",
    "}\n",
    "\n",
    "extract_all(\"data\", \"data_small\", ratios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ed868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
