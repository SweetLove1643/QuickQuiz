import os
import logging
from typing import List, Optional, Dict, Any
from datetime import datetime
import json
from sqlite_retriever import SQLiteDocumentRetriever as DocumentRetriever
from llm_adapter import GeminiChatAdapter
from schemas import (
    RAGChatRequest,
    RAGChatResponse,
    ConversationContext,
    RetrievalConfig,
    ChatConfig,
    ConversationHistory,
)

logger = logging.getLogger(__name__)


class RAGChatEngine:
    """RAG-based chat engine káº¿t há»£p document retrieval vÃ  Gemini LLM."""

    def __init__(
        self,
        retriever: Optional[DocumentRetriever] = None,
        llm_adapter: Optional[GeminiChatAdapter] = None,
    ):
        """
        Initialize RAG chat engine.

        Args:
            retriever: Document retriever instance
            llm_adapter: LLM adapter for chat generation
        """
        self.retriever = retriever or DocumentRetriever()
        self.llm_adapter = llm_adapter or GeminiChatAdapter()

        # Conversation storage (in-memory, cÃ³ thá»ƒ migrate to DB)
        self.conversations: Dict[str, ConversationHistory] = {}

    def initialize(self, force_rebuild_index: bool = False) -> None:
        """Initialize all components."""
        logger.info("Initializing RAG chat engine...")
        self.retriever.initialize(force_rebuild=force_rebuild_index)
        logger.info("RAG chat engine initialized")

    def chat(
        self, request: RAGChatRequest, conversation_id: Optional[str] = None
    ) -> RAGChatResponse:
        """
        Process chat request sá»­ dá»¥ng RAG approach.

        Args:
            request: Chat request vá»›i query vÃ  configs
            conversation_id: Optional conversation ID for context

        Returns:
            Chat response with answer vÃ  context
        """
        start_time = datetime.now()

        try:
            # Debug log
            logger.info(f"=== CHAT REQUEST ===")
            logger.info(f"Query: '{request.query}'")
            logger.info(f"Conversation ID: {conversation_id}")
            
            # 1. Retrieve relevant documents
            logger.info(f"ðŸ” Attempting to retrieve documents...")
            
            # âœ… FIX: Check if retriever has the method
            if not hasattr(self.retriever, 'retrieve_documents'):
                logger.error("âŒ Retriever does not have retrieve_documents method!")
                logger.info(f"ðŸ“¦ Retriever type: {type(self.retriever)}")
                logger.info(f"ðŸ“¦ Retriever methods: {dir(self.retriever)}")
                retrieved_docs = []
            else:
                try:
                    retrieved_docs = self.retriever.retrieve_documents(
                        query=request.query, config=request.retrieval_config
                    )
                except Exception as retrieve_err:
                    logger.error(f"âŒ Error retrieving documents: {retrieve_err}", exc_info=True)
                    retrieved_docs = []
            
            logger.info(f"ðŸ“Š Retrieved {len(retrieved_docs)} documents")
            
            # DEBUG: Check if chunks exist at all
            total_chunks = self.retriever.get_document_count()
            logger.info(f"ðŸ“ˆ Total documents available in system: {total_chunks}")
            
            if len(retrieved_docs) == 0:
                logger.warning(f"âš ï¸ No documents retrieved for query: '{request.query}'")
                logger.info(f"Retrieval config: {request.retrieval_config}")
                # Try a broader search
                logger.info("ðŸ”„ Trying fallback: search all documents...")
                fallback_config = RetrievalConfig(top_k=10)
                fallback_docs = self.retriever.retrieve_documents(
                    query="document", config=fallback_config
                )
                logger.info(f"Fallback search returned: {len(fallback_docs)} documents")
            
            for i, doc in enumerate(retrieved_docs):
                logger.info(
                    f"  Doc {i+1}: {doc.topic} (score: {doc.similarity_score:.3f}, content_len: {len(doc.content)})"
                )

            # 2. Build context tá»« retrieved documents
            context = self._build_context(retrieved_docs, request.chat_config)

            # 3. Get conversation history if available
            conversation_history = self._get_conversation_history(conversation_id)

            # 4. Generate response sá»­ dá»¥ng LLM
            llm_response = self._generate_response(
                query=request.query,
                context=context,
                conversation_history=conversation_history,
                config=request.chat_config,
            )

            # 5. Store conversation
            if conversation_id:
                self._update_conversation(conversation_id, request.query, llm_response)

            # 6. Build response
            response = RAGChatResponse(
                answer=llm_response,
                context=context,
                conversation_id=conversation_id,
                timestamp=start_time.isoformat(),
                processing_time=(datetime.now() - start_time).total_seconds(),
                retrieved_documents=[doc.model_dump() for doc in retrieved_docs],
            )

            logger.info(f"Chat processed in {response.processing_time:.2f}s")
            return response

        except Exception as e:
            logger.error(f"Error processing chat request: {e}")

            # Return error response
            return RAGChatResponse(
                answer=f"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n: {str(e)}",
                context=ConversationContext(
                    retrieved_count=0, context_used=False, sources=[]
                ),
                conversation_id=conversation_id,
                timestamp=start_time.isoformat(),
                processing_time=(datetime.now() - start_time).total_seconds(),
                retrieved_documents=[],
            )

    def _build_context(
        self, retrieved_docs: List, config: ChatConfig
    ) -> ConversationContext:
        """Build context object tá»« retrieved documents."""
        if not retrieved_docs:
            return ConversationContext(
                retrieved_count=0, context_used=False, sources=[]
            )

        # Collect sources
        sources = []
        context_text_parts = []

        for doc in retrieved_docs[: config.max_context_docs]:
            # Add to sources
            sources.append(
                {
                    "document_id": doc.document_id,
                    "topic": doc.topic,
                    "category": doc.category,
                    "similarity_score": doc.similarity_score,
                    "chunk_text": (
                        getattr(doc, "chunk_text", doc.content or "")[:200] + "..."
                        if len(getattr(doc, "chunk_text", doc.content or "")) > 200
                        else getattr(doc, "chunk_text", doc.content or "")
                    ),
                }
            )

            # Add to context text
            chunk_text = getattr(doc, "chunk_text", doc.content)
            context_text_parts.append(f"[{doc.topic}] {chunk_text}")

        return ConversationContext(
            retrieved_count=len(retrieved_docs),
            context_used=len(sources) > 0,
            sources=sources,
            context_text=(
                "\n\n".join(context_text_parts) if context_text_parts else None
            ),
        )

    def _get_conversation_history(
        self, conversation_id: Optional[str]
    ) -> Optional[List[Dict[str, str]]]:
        """Get conversation history for context."""
        if not conversation_id or conversation_id not in self.conversations:
            return None

        history = self.conversations[conversation_id]

        # Return recent messages for context (limit to avoid token overflow)
        recent_messages = history.messages[-6:]  # Last 6 messages (3 exchanges)

        formatted_history = []
        for msg in recent_messages:
            formatted_history.append({"role": msg["role"], "content": msg["content"]})

        return formatted_history

    def _generate_response(
        self,
        query: str,
        context: ConversationContext,
        conversation_history: Optional[List[Dict[str, str]]],
        config: ChatConfig,
    ) -> str:
        """Generate response sá»­ dá»¥ng Gemini LLM."""

        # Build prompt
        system_prompt = self._build_system_prompt(config)
        user_prompt = self._build_user_prompt(query, context, config)

        # Prepare messages
        messages = [{"role": "system", "content": system_prompt}]

        # Add conversation history if available
        if conversation_history:
            messages.extend(conversation_history)

        # Add current user query
        messages.append({"role": "user", "content": user_prompt})

        # Generate response
        response = self.llm_adapter.generate_response(messages, config)
        return response

    def _build_system_prompt(self, config: ChatConfig) -> str:
        """Build system prompt cho Gemini."""

        base_prompt = """Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh chuyÃªn tráº£ lá»i cÃ¢u há»i dá»±a trÃªn cÃ¡c tÃ i liá»‡u vÃ  tÃ³m táº¯t Ä‘Æ°á»£c cung cáº¥p.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn thÃ´ng tin tá»« cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c truy xuáº¥t
- ÄÆ°a ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c, há»¯u Ã­ch vÃ  dá»… hiá»ƒu
- Sá»­ dá»¥ng tiáº¿ng Viá»‡t má»™t cÃ¡ch tá»± nhiÃªn vÃ  thÃ¢n thiá»‡n
- TrÃ­ch dáº«n thÃ´ng tin tá»« cÃ¡c nguá»“n tÃ i liá»‡u khi cÃ³ thá»ƒ

QUY Táº®C:
1. Æ¯u tiÃªn sá»­ dá»¥ng thÃ´ng tin tá»« cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p
2. Náº¿u khÃ´ng cÃ³ thÃ´ng tin liÃªn quan, hÃ£y thÃ nh tháº­t nÃ³i ráº±ng báº¡n khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p
3. KhÃ´ng táº¡o ra thÃ´ng tin khÃ´ng cÃ³ trong tÃ i liá»‡u
4. TrÃ¬nh bÃ y cÃ¢u tráº£ lá»i cÃ³ cáº¥u trÃºc, dá»… Ä‘á»c
5. Sá»­ dá»¥ng bullet points hoáº·c danh sÃ¡ch khi phÃ¹ há»£p"""

        if config.include_sources:
            base_prompt += """
6. LuÃ´n Ä‘á» cáº­p Ä‘áº¿n nguá»“n thÃ´ng tin khi tráº£ lá»i (vÃ­ dá»¥: "Theo tÃ i liá»‡u vá» Python...")"""

        if config.response_style:
            base_prompt += f"""
7. Phong cÃ¡ch tráº£ lá»i: {config.response_style}"""

        return base_prompt

    def _build_user_prompt(
        self, query: str, context: ConversationContext, config: ChatConfig
    ) -> str:
        """Build user prompt vá»›i context vÃ  query."""

        if not context.context_used or not context.context_text:
            return f"""CÃ¢u há»i: {query}

LÆ°u Ã½: TÃ´i khÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u nÃ o liÃªn quan Ä‘áº¿n cÃ¢u há»i nÃ y trong cÆ¡ sá»Ÿ dá»¯ liá»‡u."""

        prompt_parts = [
            "=== TÃ€I LIá»†U THAM KHáº¢O ===",
            context.context_text,
            "",
            "=== CÃ‚U Há»ŽI ===",
            query,
        ]

        if config.include_sources:
            prompt_parts.extend(
                [
                    "",
                    "HÃ£y tráº£ lá»i dá»±a trÃªn cÃ¡c tÃ i liá»‡u trÃªn vÃ  Ä‘á» cáº­p Ä‘áº¿n nguá»“n thÃ´ng tin.",
                ]
            )
        else:
            prompt_parts.extend(["", "HÃ£y tráº£ lá»i dá»±a trÃªn cÃ¡c tÃ i liá»‡u trÃªn."])

        return "\n".join(prompt_parts)

    def _update_conversation(
        self, conversation_id: str, query: str, response: str
    ) -> None:
        """Update conversation history."""

        if conversation_id not in self.conversations:
            self.conversations[conversation_id] = ConversationHistory(
                conversation_id=conversation_id,
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat(),
                messages=[],
            )

        conversation = self.conversations[conversation_id]

        # Add user message
        conversation.messages.append(
            {"role": "user", "content": query, "timestamp": datetime.now().isoformat()}
        )

        # Add assistant response
        conversation.messages.append(
            {
                "role": "assistant",
                "content": response,
                "timestamp": datetime.now().isoformat(),
            }
        )

        conversation.updated_at = datetime.now().isoformat()

    def get_conversation(self, conversation_id: str) -> Optional[ConversationHistory]:
        """Get conversation by ID."""
        return self.conversations.get(conversation_id)

    def list_conversations(self, limit: int = 50) -> List[ConversationHistory]:
        """List recent conversations."""
        conversations = list(self.conversations.values())
        conversations.sort(key=lambda x: x.updated_at, reverse=True)
        return conversations[:limit]

    def delete_conversation(self, conversation_id: str) -> bool:
        """Delete conversation by ID."""
        if conversation_id in self.conversations:
            del self.conversations[conversation_id]
            return True
        return False

    def get_stats(self) -> Dict[str, Any]:
        """Get chat engine statistics."""
        retriever_stats = self.retriever.get_stats()

        return {
            "retriever": retriever_stats,
            "conversations": {
                "total_conversations": len(self.conversations),
                "total_messages": sum(
                    len(conv.messages) for conv in self.conversations.values()
                ),
            },
            "llm_adapter": {
                "model": getattr(self.llm_adapter, "current_model", "unknown"),
                "status": "ready",
            },
        }


# Global chat engine instance (singleton pattern)
_chat_engine = None


def get_chat_engine(retriever: Optional[DocumentRetriever] = None) -> RAGChatEngine:
    """
    Get global chat engine instance.

    Args:
        retriever: Optional document retriever

    Returns:
        RAGChatEngine instance
    """
    global _chat_engine

    if _chat_engine is None:
        _chat_engine = RAGChatEngine(retriever=retriever)

    return _chat_engine


# Add get_stats method to RAGChatEngine
def _add_stats_method():
    def get_stats(self) -> Dict[str, Any]:
        """Get chat engine statistics."""
        try:
            retriever_stats = (
                self.retriever.get_stats()
                if hasattr(self.retriever, "get_stats")
                else {}
            )

            return {
                "conversations": len(self.conversations),
                "retriever_stats": retriever_stats,
                "engine_initialized": True,
            }
        except Exception as e:
            logger.error(f"Error getting chat engine stats: {e}")
            return {"error": str(e)}

    RAGChatEngine.get_stats = get_stats


# Apply the stats method
_add_stats_method()


__all__ = ["RAGChatEngine", "get_chat_engine"]
