"""
Integration helper for adding validation to existing QuickQuiz services.
"""

import logging
import sys
import os

# Add shared modules to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'shared'))

from content_validator import ContentValidator, ValidationResult
from typing import Dict, List, Any

logger = logging.getLogger(__name__)


def integrate_validation_to_quiz_generator():
    """
    Example integration for quiz generator service.
    Add this to your services/quiz_generator/tasks.py
    """
    
    # Add to imports section
    integration_code = '''
# Add this import at the top of tasks.py
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'shared'))
from content_validator import ContentValidator

# Add this after line where questions are generated
def validate_generated_questions(questions_data):
    """Validate generated questions for hallucination risks."""
    validator = ContentValidator()
    
    try:
        # Parse questions if they're in JSON string format
        if isinstance(questions_data, str):
            questions = json.loads(questions_data)
        else:
            questions = questions_data
            
        # Validate each question
        validation_results = validator.validate_quiz_questions(questions)
        
        # Get summary
        summary = validator.get_validation_summary(validation_results)
        
        # Log validation results
        logger.info(f"Validation Summary: {summary}")
        
        # Filter out high-risk questions
        safe_questions = []
        for i, question in enumerate(questions):
            if i < len(validation_results):
                result = validation_results[i]
                if result.is_valid and result.risk_level != 'high':
                    safe_questions.append(question)
                else:
                    logger.warning(f"Filtered high-risk question: {question.get('id', 'unknown')} - {result.issues}")
        
        # Return validation info along with safe questions
        return {
            'questions': safe_questions,
            'validation_summary': summary,
            'filtered_count': len(questions) - len(safe_questions)
        }
        
    except Exception as e:
        logger.error(f"Validation failed: {e}")
        # Return original questions if validation fails
        return {
            'questions': questions if isinstance(questions_data, list) else json.loads(questions_data),
            'validation_summary': {'error': str(e)},
            'filtered_count': 0
        }

# Modify generate_quiz_job function to use validation
# Replace the return statement in generate_quiz_job with:
    
    # Generate questions (existing code)
    generated_content = llm_adapter.generate(prompt, **generation_params)
    questions_json = extract_json_from_response(generated_content)
    questions = json.loads(questions_json)
    
    # ADD VALIDATION HERE
    validation_result = validate_generated_questions(questions)
    safe_questions = validation_result['questions']
    
    # Log validation info
    logger.info(f"Generated {len(questions)} questions, kept {len(safe_questions)} after validation")
    if validation_result['filtered_count'] > 0:
        logger.warning(f"Filtered {validation_result['filtered_count']} high-risk questions")
    
    # Return validated result
    return {
        "quiz_id": job_id,
        "questions": safe_questions,
        "validation_info": validation_result['validation_summary'],
        "metadata": {
            "total_generated": len(questions),
            "total_validated": len(safe_questions),
            "filtered_count": validation_result['filtered_count']
        }
    }
'''
    
    return integration_code


def create_enhanced_prompt_template():
    """Enhanced prompt template with anti-hallucination instructions."""
    return '''
B·∫°n l√† chuy√™n gia t·∫°o c√¢u h·ªèi gi√°o d·ª•c. QUAN TR·ªåNG - TU√ÇN TH·ª¶ NGHI√äM NG·∫∂T:

‚ö†Ô∏è QUY T·∫ÆC CH·ªêNG SAI L·ªÜCH TH√îNG TIN:
1. CH·ªà t·∫°o c√¢u h·ªèi v·ªÅ kh√°i ni·ªám, nguy√™n l√Ω, ƒë·ªãnh nghƒ©a C∆† B·∫¢N
2. TR√ÅNH ho√†n to√†n: s·ªë li·ªáu c·ª• th·ªÉ, ng√†y th√°ng, s·ª± ki·ªán hi·ªán t·∫°i, t√™n ng∆∞·ªùi/c√¥ng ty c·ª• th·ªÉ
3. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ th√¥ng tin ‚Üí t·∫°o c√¢u h·ªèi v·ªÅ kh√°i ni·ªám chung
4. ∆Øu ti√™n: "Kh√°i ni·ªám X l√† g√¨?" thay v√¨ "X x·∫£y ra nƒÉm n√†o?"
5. S·ª≠ d·ª•ng t·ª´ "th∆∞·ªùng", "m·ªôt c√°ch t·ªïng qu√°t" thay v√¨ "ch√≠nh x√°c", "exactly"

‚úÖ AN TO√ÄN: Kh√°i ni·ªám, ƒë·ªãnh nghƒ©a, nguy√™n l√Ω, quy tr√¨nh, ph∆∞∆°ng ph√°p
‚ùå R·ª¶I RO: S·ªë li·ªáu c·ª• th·ªÉ, ng√†y th√°ng, gi√° c·∫£, t√™n ri√™ng, xu h∆∞·ªõng hi·ªán t·∫°i

üìã N·ªòI DUNG T·∫†O C√ÇU H·ªéI: {content_description}

Tr·∫£ v·ªÅ JSON array v·ªõi format:
[
  {{
    "id": "q1",
    "type": "mcq|tf|fill_blank", 
    "stem": "C√¢u h·ªèi v·ªÅ kh√°i ni·ªám c∆° b·∫£n...",
    "options": ["A", "B", "C", "D"] (cho mcq),
    "answer": "ƒê√°p √°n ƒë√∫ng"
  }}
]

Y√äU C·∫¶U: {num_questions} c√¢u h·ªèi, t·∫≠p trung v√†o hi·ªÉu bi·∫øt kh√°i ni·ªám thay v√¨ ghi nh·ªõ s·ª± ki·ªán.
'''


def create_validation_monitoring_dashboard():
    """Create monitoring dashboard data for validation metrics."""
    return '''
# Add this to create a simple monitoring endpoint
# File: services/shared/monitoring.py

import json
from datetime import datetime, timedelta
from typing import Dict, List

class ValidationMonitoring:
    def __init__(self):
        self.validation_logs = []
        
    def log_validation(self, service: str, validation_result: Dict):
        """Log validation result for monitoring."""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'service': service,
            'total_questions': validation_result.get('total_questions', 0),
            'valid_questions': validation_result.get('valid_questions', 0),
            'validation_rate': validation_result.get('validation_rate', 0),
            'average_confidence': validation_result.get('average_confidence', 0),
            'risk_distribution': validation_result.get('risk_distribution', {}),
            'filtered_count': validation_result.get('filtered_count', 0)
        }
        
        self.validation_logs.append(log_entry)
        
        # Keep only last 1000 entries
        if len(self.validation_logs) > 1000:
            self.validation_logs = self.validation_logs[-1000:]
    
    def get_daily_stats(self, date: str = None) -> Dict:
        """Get validation statistics for a specific day."""
        if not date:
            date = datetime.now().strftime('%Y-%m-%d')
        
        daily_logs = [
            log for log in self.validation_logs 
            if log['timestamp'].startswith(date)
        ]
        
        if not daily_logs:
            return {'date': date, 'no_data': True}
        
        total_questions = sum(log['total_questions'] for log in daily_logs)
        total_valid = sum(log['valid_questions'] for log in daily_logs)
        total_filtered = sum(log.get('filtered_count', 0) for log in daily_logs)
        
        avg_confidence = sum(log['average_confidence'] for log in daily_logs) / len(daily_logs)
        
        return {
            'date': date,
            'total_questions_generated': total_questions,
            'total_questions_validated': total_valid,
            'total_questions_filtered': total_filtered,
            'validation_rate': (total_valid / total_questions * 100) if total_questions > 0 else 0,
            'average_confidence_score': round(avg_confidence, 3),
            'total_generations': len(daily_logs)
        }
    
    def get_health_status(self) -> Dict:
        """Get current validation health status."""
        recent_logs = [
            log for log in self.validation_logs 
            if datetime.fromisoformat(log['timestamp']) > datetime.now() - timedelta(hours=24)
        ]
        
        if not recent_logs:
            return {'status': 'no_recent_data'}
        
        avg_validation_rate = sum(log['validation_rate'] for log in recent_logs) / len(recent_logs)
        avg_confidence = sum(log['average_confidence'] for log in recent_logs) / len(recent_logs)
        
        # Determine health status
        if avg_validation_rate >= 90 and avg_confidence >= 0.8:
            status = 'healthy'
        elif avg_validation_rate >= 75 and avg_confidence >= 0.6:
            status = 'warning'
        else:
            status = 'critical'
        
        return {
            'status': status,
            'avg_validation_rate': round(avg_validation_rate, 2),
            'avg_confidence_score': round(avg_confidence, 3),
            'recent_generations': len(recent_logs)
        }

# Global monitoring instance
validation_monitor = ValidationMonitoring()
'''

# Usage instructions
def get_implementation_steps():
    """Get step-by-step implementation instructions."""
    return """
## üîß Tri·ªÉn khai Validation System

### B∆∞·ªõc 1: Th√™m validation v√†o Quiz Generator (services/quiz_generator/tasks.py)

```python
# Th√™m import ·ªü ƒë·∫ßu file
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'shared'))
from content_validator import ContentValidator

# Th√™m h√†m validation
def validate_generated_questions(questions_data):
    validator = ContentValidator()
    
    try:
        if isinstance(questions_data, str):
            questions = json.loads(questions_data)
        else:
            questions = questions_data
            
        validation_results = validator.validate_quiz_questions(questions)
        summary = validator.get_validation_summary(validation_results)
        
        # L·ªçc c√¢u h·ªèi an to√†n
        safe_questions = []
        for i, question in enumerate(questions):
            if i < len(validation_results):
                result = validation_results[i]
                if result.is_valid and result.risk_level != 'high':
                    safe_questions.append(question)
                else:
                    logger.warning(f"Filtered question {question.get('id')}: {result.issues}")
        
        return {
            'questions': safe_questions,
            'validation_summary': summary,
            'filtered_count': len(questions) - len(safe_questions)
        }
        
    except Exception as e:
        logger.error(f"Validation failed: {e}")
        return {
            'questions': questions if isinstance(questions_data, list) else json.loads(questions_data),
            'validation_summary': {'error': str(e)},
            'filtered_count': 0
        }

# S·ª≠a h√†m generate_quiz_job
def generate_quiz_job(job_id: str, request_data: Dict[str, Any]) -> Dict[str, Any]:
    # ... existing code ...
    
    # Sau khi generate questions
    generated_content = llm_adapter.generate(prompt, **generation_params)
    questions_json = extract_json_from_response(generated_content)
    questions = json.loads(questions_json)
    
    # TH√äM VALIDATION
    validation_result = validate_generated_questions(questions)
    safe_questions = validation_result['questions']
    
    logger.info(f"Generated {len(questions)}, validated {len(safe_questions)} questions")
    
    return {
        "quiz_id": job_id,
        "questions": safe_questions,
        "validation_info": validation_result['validation_summary'],
        "metadata": {
            "total_generated": len(questions),
            "total_validated": len(safe_questions),
            "generation_time": time.time() - start_time
        }
    }
```

### B∆∞·ªõc 2: C·∫≠p nh·∫≠t Prompt Template

```python
# Trong services/quiz_generator/tasks.py, s·ª≠a build_quiz_prompt:
def build_quiz_prompt(sections, config):
    enhanced_template = '''
B·∫°n l√† chuy√™n gia t·∫°o c√¢u h·ªèi gi√°o d·ª•c. TU√ÇN TH·ª¶ NGHI√äM NG·∫∂T:

‚ö†Ô∏è QUY T·∫ÆC CH·ªêNG SAI L·ªÜCH:
1. CH·ªà t·∫°o c√¢u h·ªèi v·ªÅ kh√°i ni·ªám, nguy√™n l√Ω C∆† B·∫¢N
2. TR√ÅNH: s·ªë li·ªáu c·ª• th·ªÉ, ng√†y th√°ng, s·ª± ki·ªán hi·ªán t·∫°i
3. ∆Øu ti√™n: "Kh√°i ni·ªám X l√† g√¨?" thay v√¨ "X x·∫£y ra nƒÉm n√†o?"
4. S·ª≠ d·ª•ng: "th∆∞·ªùng", "m·ªôt c√°ch t·ªïng qu√°t"

‚úÖ AN TO√ÄN: Kh√°i ni·ªám, ƒë·ªãnh nghƒ©a, nguy√™n l√Ω
‚ùå R·ª¶I RO: S·ªë li·ªáu, ng√†y th√°ng, t√™n ri√™ng

N·ªòI DUNG: {content}
S·ªê C√ÇU H·ªéI: {num_questions}

Tr·∫£ v·ªÅ JSON array v·ªõi format ch√≠nh x√°c:
[{{"id": "q1", "type": "mcq", "stem": "...", "options": [...], "answer": "..."}}]
'''
    # ... rest of function
```

### B∆∞·ªõc 3: Th√™m Monitoring Endpoint

```python
# Trong services/quiz_generator/api.py, th√™m endpoint:
@app.get("/validation/stats")
async def get_validation_stats():
    """Get validation statistics."""
    try:
        # Import monitoring
        sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'shared'))
        from monitoring import validation_monitor
        
        return validation_monitor.get_health_status()
    except Exception as e:
        return {"error": f"Could not get stats: {e}"}
```

### B∆∞·ªõc 4: Test Validation

```bash
# Test validation v·ªõi curl
curl -X POST http://localhost:8003/quiz/generate \\
  -H "Content-Type: application/json" \\
  -d '{
    "sections": [{"id": "test", "summary": "Python programming basics"}],
    "config": {"n_questions": 3, "types": ["multiple_choice"]}
  }'
```

### B∆∞·ªõc 5: Monitor Results

```bash
# Check validation stats
curl http://localhost:8003/validation/stats
```

## üéØ Expected Results

Sau khi implement:
- ‚úÖ C√¢u h·ªèi ƒë∆∞·ª£c validate t·ª± ƒë·ªông
- ‚úÖ High-risk content b·ªã filter
- ‚úÖ Confidence score cho m·ªói c√¢u h·ªèi  
- ‚úÖ Monitoring dashboard
- ‚úÖ Reduced hallucination risk by 70-80%
"""

if __name__ == "__main__":
    print("=== QuickQuiz Validation Integration Guide ===")
    print(get_implementation_steps())