# NgÄƒn cháº·n AI Hallucination trong QuickQuiz

## ğŸš¨ Váº¥n Ä‘á» AI Hallucination

### Äá»‹nh nghÄ©a

AI Hallucination xáº£y ra khi mÃ´ hÃ¬nh AI táº¡o ra thÃ´ng tin sai lá»‡ch, khÃ´ng chÃ­nh xÃ¡c hoáº·c bá»‹a Ä‘áº·t mÃ  váº«n trÃ¬nh bÃ y má»™t cÃ¡ch tá»± tin.

### Rá»§i ro trong QuickQuiz

1. **Quiz Generator**: Táº¡o cÃ¢u há»i vá»›i thÃ´ng tin sai, Ä‘Ã¡p Ã¡n khÃ´ng chÃ­nh xÃ¡c
2. **Quiz Evaluator**: PhÃ¢n tÃ­ch káº¿t quáº£ sai lá»‡ch, lá»i khuyÃªn khÃ´ng phÃ¹ há»£p

## ğŸ›¡ï¸ Chiáº¿n lÆ°á»£c ngÄƒn cháº·n

### 1. Content Validation Pipeline

```python
# services/shared/validators.py
class ContentValidator:
    def validate_quiz_content(self, questions):
        """Kiá»ƒm tra Ä‘á»™ tin cáº­y ná»™i dung cÃ¢u há»i"""
        validation_results = []

        for q in questions:
            result = {
                'question_id': q['id'],
                'confidence_score': 0.0,
                'issues': [],
                'suggestions': []
            }

            # 1. Fact-checking cÆ¡ báº£n
            if self._contains_specific_facts(q):
                result['issues'].append('Contains specific facts - needs verification')

            # 2. Kiá»ƒm tra logic Ä‘Ã¡p Ã¡n
            if not self._validate_answer_logic(q):
                result['issues'].append('Answer logic inconsistent')

            # 3. Kiá»ƒm tra tÃ­nh cáº­p nháº­t
            if self._contains_temporal_info(q):
                result['issues'].append('Contains time-sensitive information')

            validation_results.append(result)

        return validation_results
```

### 2. Multi-Model Consensus

```python
# services/shared/consensus.py
class ModelConsensus:
    def __init__(self):
        self.models = ['gemini-2.5-flash', 'gemini-2.5-pro']

    def generate_with_consensus(self, prompt, min_agreement=0.8):
        """Táº¡o ná»™i dung vá»›i sá»± Ä‘á»“ng thuáº­n tá»« nhiá»u model"""
        results = []

        for model in self.models:
            try:
                result = self.adapter.generate(prompt, model=model)
                results.append(result)
            except Exception as e:
                logger.warning(f"Model {model} failed: {e}")

        # So sÃ¡nh káº¿t quáº£ vÃ  tÃ­nh Ä‘á»™ Ä‘á»“ng thuáº­n
        consensus_score = self._calculate_consensus(results)

        if consensus_score >= min_agreement:
            return self._merge_results(results)
        else:
            raise ValidationError(f"Low consensus score: {consensus_score}")
```

### 3. Prompt Engineering Anti-Hallucination

```python
# services/quiz_generator/prompts.py
def get_anti_hallucination_prompt():
    return """
Báº¡n lÃ  chuyÃªn gia táº¡o cÃ¢u há»i giÃ¡o dá»¥c. QUAN TRá»ŒNG:

âš ï¸ QUY Táº®C CHá»NG SAI Lá»†CH THÃ”NG TIN:
1. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin báº¡n CHáº®C CHáº®N lÃ  Ä‘Ãºng
2. Náº¿u KHÃ”NG CHáº®C vá» thÃ´ng tin, hÃ£y táº¡o cÃ¢u há»i vá» khÃ¡i niá»‡m chung thay vÃ¬ sá»± kiá»‡n cá»¥ thá»ƒ
3. Vá»›i cÃ¢u há»i vá» sá»‘ liá»‡u, ngÃ y thÃ¡ng, sá»± kiá»‡n lá»‹ch sá»­: hÃ£y ghi chÃº "Cáº§n kiá»ƒm tra thÃ´ng tin"
4. Æ¯u tiÃªn cÃ¢u há»i vá» nguyÃªn lÃ½, khÃ¡i niá»‡m, logic thay vÃ¬ sá»± kiá»‡n cá»¥ thá»ƒ
5. Äá»‘i vá»›i thÃ´ng tin khoa há»c/ká»¹ thuáº­t: chá»‰ sá»­ dá»¥ng kiáº¿n thá»©c cÆ¡ báº£n Ä‘Æ°á»£c cÃ´ng nháº­n rá»™ng rÃ£i

ğŸ“‹ TEMPLATE AN TOÃ€N:
- "KhÃ¡i niá»‡m X cÃ³ Ä‘áº·c Ä‘iá»ƒm gÃ¬?" (thay vÃ¬ "X Ä‘Æ°á»£c phÃ¡t minh nÄƒm nÃ o?")
- "NguyÃªn lÃ½ Y hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?" (thay vÃ¬ "CÃ´ng ty Z cÃ³ bao nhiÃªu nhÃ¢n viÃªn?")
- "Äiá»u gÃ¬ xáº£y ra khi..." (thay vÃ¬ "VÃ o ngÃ y DD/MM/YYYY Ä‘iá»u gÃ¬ Ä‘Ã£ xáº£y ra?")

ğŸ¯ Ná»˜I DUNG Cáº¦N Táº O: {content_topic}
"""
```

### 4. Real-time Fact Checking

```python
# services/shared/fact_checker.py
class FactChecker:
    def __init__(self):
        self.knowledge_bases = {
            'wikipedia_api': 'https://en.wikipedia.org/api/rest_v1/',
            'wolfram_alpha': 'http://api.wolframalpha.com/v2/',
        }

    def verify_factual_claims(self, question_data):
        """Kiá»ƒm tra tÃ­nh chÃ­nh xÃ¡c cá»§a thÃ´ng tin trong cÃ¢u há»i"""
        claims = self._extract_claims(question_data)
        verification_results = []

        for claim in claims:
            result = {
                'claim': claim,
                'verified': False,
                'confidence': 0.0,
                'sources': []
            }

            # Kiá»ƒm tra vá»›i cÃ¡c nguá»“n Ä‘Ã¡ng tin cáº­y
            if self._check_against_knowledge_base(claim):
                result['verified'] = True
                result['confidence'] = 0.85

            verification_results.append(result)

        return verification_results
```

### 5. Human-in-the-loop Review

```python
# services/shared/review_queue.py
class ReviewQueue:
    def __init__(self):
        self.review_criteria = {
            'high_risk_topics': ['medicine', 'law', 'finance', 'current_events'],
            'confidence_threshold': 0.7,
            'consensus_threshold': 0.8
        }

    def flag_for_review(self, content, metadata):
        """ÄÃ¡nh dáº¥u ná»™i dung cáº§n review thá»§ cÃ´ng"""
        risk_score = self._calculate_risk_score(content, metadata)

        if risk_score > self.review_criteria['confidence_threshold']:
            return {
                'requires_review': True,
                'priority': 'high' if risk_score > 0.9 else 'medium',
                'reasons': self._get_risk_reasons(content),
                'suggested_reviewers': self._get_expert_reviewers(content)
            }

        return {'requires_review': False}
```

## ğŸ” Detection Methods

### 1. Confidence Scoring

```python
def calculate_confidence_score(response_data, model_metadata):
    """TÃ­nh Ä‘iá»ƒm tin cáº­y cá»§a response"""
    factors = {
        'response_length': len(response_data.get('text', '')),
        'specific_claims': count_specific_facts(response_data),
        'model_temperature': model_metadata.get('temperature', 0.7),
        'prompt_specificity': analyze_prompt_specificity(model_metadata.get('prompt'))
    }

    # Logic tÃ­nh toÃ¡n confidence score
    confidence = base_score * modifier_factors
    return min(confidence, 1.0)
```

### 2. Inconsistency Detection

```python
def detect_inconsistencies(quiz_questions):
    """PhÃ¡t hiá»‡n mÃ¢u thuáº«n trong bá»™ cÃ¢u há»i"""
    inconsistencies = []

    for i, q1 in enumerate(quiz_questions):
        for j, q2 in enumerate(quiz_questions[i+1:], i+1):
            if self._questions_contradict(q1, q2):
                inconsistencies.append({
                    'question_1': q1['id'],
                    'question_2': q2['id'],
                    'type': 'contradiction',
                    'description': self._describe_contradiction(q1, q2)
                })

    return inconsistencies
```

## ğŸ“Š Monitoring & Logging

### 1. Hallucination Metrics

```python
# services/shared/metrics.py
class HallucinationMetrics:
    def track_metrics(self):
        return {
            'daily_flagged_content': self._count_flagged_today(),
            'verification_success_rate': self._get_verification_rate(),
            'human_override_rate': self._get_override_rate(),
            'confidence_score_distribution': self._get_confidence_distribution()
        }
```

### 2. Audit Trail

```python
# services/shared/audit.py
class ContentAudit:
    def log_generation_process(self, content_id, process_data):
        """Ghi log quÃ¡ trÃ¬nh táº¡o ná»™i dung"""
        audit_entry = {
            'content_id': content_id,
            'timestamp': datetime.now(),
            'model_used': process_data['model'],
            'prompt_hash': hashlib.sha256(process_data['prompt'].encode()).hexdigest(),
            'confidence_score': process_data['confidence'],
            'validation_results': process_data['validations'],
            'review_status': process_data.get('review_status', 'pending')
        }

        self.audit_db.insert(audit_entry)
```

## ğŸ¯ Implementation Checklist

### Giai Ä‘oáº¡n 1: Foundation (Tuáº§n 1-2)

- [ ] Táº¡o ContentValidator class
- [ ] Implement confidence scoring
- [ ] Setup audit logging
- [ ] Add hallucination detection metrics

### Giai Ä‘oáº¡n 2: Advanced Validation (Tuáº§n 3-4)

- [ ] Multi-model consensus system
- [ ] Fact-checking integration
- [ ] Inconsistency detection
- [ ] Review queue system

### Giai Ä‘oáº¡n 3: Monitoring (Tuáº§n 5-6)

- [ ] Dashboard cho hallucination metrics
- [ ] Alert system cho high-risk content
- [ ] A/B testing cho validation methods
- [ ] Performance optimization

## ğŸš¦ Quality Gates

### Before Production Deployment

1. âœ… Confidence score > 0.75 cho táº¥t cáº£ cÃ¢u há»i
2. âœ… Multi-model consensus > 80%
3. âœ… Zero high-risk content flags
4. âœ… Human review completed cho medium-risk content

### Runtime Monitoring

1. ğŸ“Š Track hallucination detection rate
2. ğŸ”” Alert náº¿u confidence score giáº£m dÆ°á»›i threshold
3. ğŸ“ˆ Monitor user feedback vá» content quality
4. ğŸ” Regular audit cá»§a flagged content

---

ğŸ’¡ **Remember**: Má»¥c tiÃªu khÃ´ng pháº£i lÃ  loáº¡i bá» hoÃ n toÃ n rá»§i ro, mÃ  lÃ  giáº£m thiá»ƒu tá»‘i Ä‘a vÃ  cÃ³ há»‡ thá»‘ng phÃ¡t hiá»‡n ká»‹p thá»i.
