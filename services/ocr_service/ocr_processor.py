import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
from typing import List
import logging

logger = logging.getLogger(__name__)


class OCRProcessor:
    def __init__(self):
        self.device = self._get_device()
        self.dtype = torch.float16 if self.device == "cuda" else torch.float32
        self.model_id = "Qwen/Qwen2-VL-2B-Instruct"

        logger.info("ğŸ”„ Loading Qwen2-VL model... Please wait.")
        self._load_model()

    def _get_device(self):
        if torch.cuda.is_available():
            logger.info("ğŸš€ GPU CUDA detected â†’ using GPU")
            return "cuda"
        logger.info("ğŸ–¥ Running on CPU")
        return "cpu"

    def _load_model(self):
        try:
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.model_id, torch_dtype="auto", device_map="auto"
            )
            self.model.eval()

            self.processor = AutoProcessor.from_pretrained(self.model_id)

            logger.info("âœ… Qwen2-VL model loaded successfully")

        except Exception as e:
            logger.error(f"âŒ Error loading model: {e}")
            raise e

    async def extract_text(self, images: List[Image.Image]) -> str:
        """Extract text from a list of PIL Images"""
        try:
            # Create messages for the model
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": """HÃ£y trÃ­ch xuáº¥t toÃ n bá»™ ná»™i dung vÄƒn báº£n cÃ³ trong táº¥t cáº£ cÃ¡c áº£nh Ä‘Æ°á»£c cung cáº¥p.

YÃŠU Cáº¦U:
- TrÃ­ch xuáº¥t chÃ­nh xÃ¡c tá»«ng kÃ½ tá»±, tá»« ngá»¯
- Giá»¯ nguyÃªn format, xuá»‘ng dÃ²ng nhÆ° trong áº£nh gá»‘c
- KhÃ´ng thÃªm, bá»›t, sá»­a Ä‘á»•i ná»™i dung
- Náº¿u cÃ³ nhiá»u áº£nh, gá»™p ná»™i dung theo thá»© tá»±
- Náº¿u khÃ´ng Ä‘á»c Ä‘Æ°á»£c, ghi "KhÃ´ng thá»ƒ Ä‘á»c Ä‘Æ°á»£c ná»™i dung"

Chá»‰ tráº£ vá» ná»™i dung vÄƒn báº£n Ä‘Ã£ trÃ­ch xuáº¥t, khÃ´ng cáº§n giáº£i thÃ­ch thÃªm.""",
                        }
                    ]
                    + [{"type": "image", "image": img} for img in images],
                }
            ]

            # Prepare inputs
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            image_inputs, video_inputs = process_vision_info(messages)

            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to(self.device)

            # Generate response
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs, max_new_tokens=2048, temperature=0.1, do_sample=False
                )

            # Decode response
            generated_ids_trimmed = [
                out_ids[len(in_ids) :]
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]

            output_text = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False,
            )[0]

            return output_text.strip()

        except Exception as e:
            logger.error(f"Error in text extraction: {e}")
            raise e
